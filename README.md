# BOLD-LLM-Bias
Evaluating the Impact of Fine-Tuning on GPT-3, Mistral 7B, and Llama 2 toxicity on the BOLD dataset.

## Description
A measurement within the Hugging Face evaluate-measurement library that aims to quantify the degree of toxic, harmful, offensive, or inappropriate content of an input text using a pre-trained hate speech classification metric.

Features evaluated for toxicity:
* Gender
* Race
* Profession
* Political Ideology
* Religious Ideology


## Sources Cited
### Data
[1] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21). Association for Computing Machinery, New York, NY, USA, 862–872. https://doi.org/10.1145/3442188.3445924

### Methodology
[2] Debora Nozza, Federico Bianchi, and Dirk Hovy. 2021. HONEST: Measuring Hurtful Sentence Completion in Language Models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2398–2406, Online. Association for Computational Linguistics.
### Coding Assistance
- https://replicate.com/docs/get-started/python
